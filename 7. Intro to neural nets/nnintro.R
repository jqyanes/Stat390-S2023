#Code provided by teacher

#making function my.eval1.nnet from inputs of Xrow, w0, hidden (units in the neural network), output (number of outputs)
my.eval1.nnet = function(Xrow,w0,hidden,output) {
  #calculating number of rows in X matrix and storing it in input.layer.length
  input.layer.length<-length(Xrow)
  #creating w01 by extracting stuff from w0 that are from indexes generated by [c(1:(length(Xrow)*hidden))]
  w01<-w0[c(1:(length(Xrow)*hidden))]
  #creating w02 by excluding stuff from w0 that are from indexes generated by [c(1:(length(Xrow)*hidden))]
  w02<-w0[-c(1:(length(Xrow)*hidden))]
  #creating a matrix called w1 from w01 with input.layer.length,hidden as dimensions
  w1<-matrix(w01,input.layer.length,hidden)
  #printing w1
  print(w1)
  #printing Xrow
  print(Xrow)
  #printing w02
  print(w02)
  #multiplying matrices Xrow and w1 and storing it in xhidden
  xhidden<-t(Xrow)%*%w1
  #creating zhidden from results of logistic applied to xhidden
  zhidden<-my.logistic(xhidden)
  #assigning the products of w02 x xhidden and the sum of them to out
  out<-sum(w02*zhidden)
  #creating if conditional that applies logistic function to out if output==binary
  if(output=="binary") {
    out<-my.logistic(out)
  }
  out
}

#making function my.eval2.nnet from inputs of Xrow, w0, hidden (units in the neural network), output (number of outputs)
my.eval2.nnet<-function(w0,X,Y,hidden,output) {
  #creating zfunc argument that takes input of V and uses my.eval1.nnet
  zfunc<- function(V){my.eval1.nnet(V,w0,hidden,output)}
  #creating pred by applying zfunc to each row of X
  pred<-apply(X,1,zfunc)
  #creating if conditional that stores the cross-entropy cost in llik, if the output is binary
  if(output=="binary") {
    llik<-(-1)*sum(log(pred)*Y+log(1-pred)*(1-Y))
  }
  else {
    #calculating sum of squared differences and storing it in pred
    llik<-sum((pred-Y)^2)
  }
  #making list from llik, pred, and y
  list(llik=llik,pred=pred,y=Y)
}

#creating my logistic function
my.logistic<-function(z) {exp(z)/(1+exp(z))}

#creating a neural net model via function with variables x, y, hidden layer size = 3, and no activation for the outside layer (linear)
my.neuralnet<-function(X1,Y,hidden=3,output="linear") {
  #making a new matrix X from column of ones and x1 dataset
  X<-cbind(1,X1)
  #calculating number of columns in X matrix and storing it in input.layer.length
  input.layer.length<-length(X[1,])
  #generating random numbers with a length of input.layer.length*hidden
  w01<-rnorm(input.layer.length*hidden)
  #generating random numbers with a length of hidden
  w02<-rnorm(hidden)
  #makes w01 and w02 into a single vector called w0
  w0<-c(w01,w02)
  #prints w0
  print(w0)
  #creates new function called myoptfunc
  myoptfunc<- function(w0) {my.eval2.nnet(w0,X,Y,hidden,output)$llik}
  #numerically optimizing w0 with optim, myoptfunc, via conjugate gradient
  dum<-optim(w0,myoptfunc,method="CG")
  #storing data from par column in dum df to wfinal
  wfinal<-dum$par
  #storing data from val column in dum df toss
  ss<-dum$val
  #making predictions using the neural network and storing it in pred
  pred<-my.eval2.nnet(wfinal,X,Y,hidden,output)$pred
  plot(pred,Y)
  #creating a list with ss and wfinal
  list(ss=ss,wfinal=wfinal)
}

#the code on slide four generalizes the code on slide 3 because it allows us to specify the number of hidden layers and it regularizes via lambda

my.eval1.nnet.ml = function(Xrow,w0,hidden,output,num.layers=1) {
  input.layer.length<-length(Xrow)
  w01<-w0[c(1:(length(Xrow)*hidden))]
  w0A<-w0[-c(1:(length(Xrow)*hidden))]
  w1<-matrix(w01,input.layer.length,hidden)
  xhidden<-t(Xrow)%*%w1
  zhidden<-my.logistic(xhidden)
  if(num.layers==1) {
    w02<-w0A
  }
  else {
    nlayers<-num.layers
    while(nlayers>1) {
      w01<-w0A[c(1:(hidden*hidden))]
      w0A<-w0A[-c(1:(hidden*hidden))]
      w01<-matrix(w01,hidden,hidden)
      xhidden<-(zhidden)%*%w01
      zhidden<-my.logistic(xhidden)
      nlayers<-nlayers-1
    }
  }
  w02<-w0A
  out<-sum(w02*zhidden)
  if(output=="binary") {
    out<-my.logistic(out)
  }
  out
}

my.eval2.nnet.ml = function(w0,X,Y,hidden,output,num.layers,lambda) {
  zfunc<-function(V) {my.eval1.nnet.ml(V,w0,hidden,output,num.layers)}
  pred<-apply(X,1,zfunc)
  if(output=="binary") {
    llik<-(-1)*sum(log(pred)*Y+log(1-pred)*(1-Y))
  }
  else {
    llik<-sum((pred-Y)^2)
  }
  loglik<-llik
  llik<-llik+lambda*sum(w0^2)
  list(llik=llik,pred=pred,y=Y,loglik=loglik)
}

my.neuralnet.multilayer = function(X1,Y,hidden=3,output="linear",num.layers=1,lambda=0) {
  X<-cbind(1,X1)
  input.layer.length<-length(X[1,])
  w01<-rnorm(input.layer.length*hidden+(num.layers-1)*hidden*hidden)
  w02<-rnorm(hidden)
  w0<-c(w01,w02)
  #print(w0)
  myoptfunc = function(w0) {
    my.eval2.nnet.ml(w0,X,Y,hidden,output,num.layers,lambda)$llik
  }
  dum<-optim(w0,myoptfunc,method="CG")
  wfinal<-dum$par
  ss<-dum$val
  duh<-my.eval2.nnet.ml(wfinal,X,Y,hidden,output,num.layers,lambda)
  plot(duh$pred,Y,main=paste("llik=",duh$llik,"\nSS=",ss),xlab="Prediction")
  list(ss=ss,wfinal=wfinal,ll=duh$llik)
}



#change file pathway as needed
NOAA.newA = read.csv("C:/Users/yanes/Downloads/R/NOAAnewA.csv")
#the missing two years from NOAAnewA
byears = data.frame (X = c(42, 43),
year = c(4.1, 4.2),
X.disaster = c(20, 18), #data taken from NOAAnew
delta.temp = c(0.85, 0.89), #data taken from NOAAnew
NOAA0.year.2 = year^2,
NOAA0.delta.temp.2 = delta.temp^2,
NOAA0.year...NOAA0.delta.temp = year*delta.temp)
#combining NOAAnewA and the two years to make NOAAnewB
NOAA.newB = rbind(NOAAnewA, byears)

my.greedy.neuralnet = function(X0,Y,hidden=3,output="linear",num.layers=1,lambda=0,trials=15) {
  par(mfrow=c(4,4))
  dum1 = my.neuralnet.multilayer(X0,Y,hidden,output,num.layers,lambda)
  print(c(dum1$ss,dum1$ll))
  for(i in 1:(trials-1)) {
    dum2 = my.neuralnet.multilayer(X0,Y,hidden,output,num.layers,lambda)
    print(c(dum2$ss,dum2$ll))
    if(dum2$ss<dum1$ss) {
      dum1<-dum2
    }
  }
  dum0 = my.eval2.nnet.ml(dum1$wfinal,cbind(1,X0),Y,hidden,output,num.layers,lambda)
  plot(dum0$pred,Y,main=paste("llik=",dum0$llik,"\nSS=",dum1$ss),xlab="Prediction")
  dum1
}

my.greedy.neuralnet(NOAA.newB[,c(-2)],NOAA.netB[,2],hidden=4,num.layers=2,lambda=0.003,63)
# Plots generally become more linear over time and the predicted values match Y (the actual values) more, indicating that the optimizer is becoming more accurate
# Sum of squares is fairly close to 0, indicating a good fit
